{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('ENV')",
   "metadata": {
    "interpreter": {
     "hash": "765dcd3d8e2bda596de27f8dfe359e7bb32436c07d19eb3e8f090ebd4be66c84"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as hub\n",
    "import numpy\n",
    "import spacy\n",
    "import re\n",
    "from utils.ngram import ngram\n",
    "from utils.create_hash import create_word_to_int, create_int_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (1):\n",
    "    rf = open(\"./data/hp0.txt\", \"r\", encoding=\"utf8\")\n",
    "    wf = open(\"./data/hp1.txt\", \"w\", encoding=\"utf8\")\n",
    "    for line in rf.readlines():\n",
    "        if (re.match(\"Page | . Harry Potter and the Philosophers Stone - J.K. Rowling\", line) == None and line != \"\\n\"):\n",
    "            wf.write(line.replace(\"\\n\", \" \"))\n",
    "    rf.close()\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_punc = open(\"./data/hp2.txt\", \"r\", encoding=\"utf8\")\n",
    "punctuation = [\".\", \"\\\"\", \"\\'\", \":\", \";\", \",\", \"”\", \"“\", \"-\", \"—\", \"!\", \"?\", \"/\", \"\\\\\", \"(\", \")\"]\n",
    "wf = open(\"./data/hp1.txt\", \"w\", encoding=\"utf8\")\n",
    "lines = clean_punc.read()\n",
    "for each in punctuation:\n",
    "    lines = lines.replace(each, \" \")\n",
    "lines = re.sub(\" +\", \" \", lines)\n",
    "wf.write(lines)\n",
    "clean_punc.close()\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_puncs(doc): # doc output is list\n",
    "    output = []\n",
    "    for token in doc:\n",
    "        if (not token.is_punct or token.text != \" \"):\n",
    "            output.append(token)\n",
    "    return output\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "rf = open(\"./data/hp1.txt\", \"r\", encoding=\"utf8\")\n",
    "doc = nlp(rf.read())\n",
    "doc = remove_puncs(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_int = create_word_to_int(doc)\n",
    "int_to_word = create_int_to_word(word_to_int)\n",
    "print(\"word_to_int and int_to_word create done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(doc))\n",
    "print(word_to_int[\"THE\"])\n",
    "print(int_to_word[6692])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ngram = ngram(doc, 7, word_to_int)\n",
    "word_ngram.shuffle()\n",
    "print(\"create n gram done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_ngram[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_ngram = embed(list(zip(*word_ngram))[0])\n",
    "labels = list(zip(*word_ngram))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_ngram))\n",
    "print(embedded_ngram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(512,))\n",
    "num_classes = len(word_to_int)\n",
    "x = layers.Dense(3000, activation=\"relu\")(inputs)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(embedded_ngram.numpy(), numpy.array(labels),\n",
    "          batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 6692)\nhad\n"
     ]
    }
   ],
   "source": [
    "embedded = embed(['Harry, What Where did the Ron'])\n",
    "outputboi = model.predict(embedded.numpy()) # 1 * 6000\n",
    "print(outputboi.shape)\n",
    "outputboi = tf.math.argmax(outputboi, axis=1)\n",
    "print(int_to_word_loaded[int(outputboi.numpy()[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('xd_loss0.4299/xd_loss0.4299')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_int_loaded = pickle.load(open(\"xd_loss0.4299/xd_loss0.4299/word2int.p\", \"rb\"))\n",
    "int_to_word_loaded = create_int_to_word(word_to_int_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}